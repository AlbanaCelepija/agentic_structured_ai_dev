{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from download_cards import download_model_cards\n",
    "from utils import *\n",
    "from huggingface_hub import login\n",
    "\n",
    "from retriever import Retriever\n",
    "from generator import Generator\n",
    "\n",
    "from opik import configure \n",
    "from opik.integrations.langchain import OpikTracer \n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_tracer = OpikTracer(project_name=\"ai_engineers_agents_project\") \n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i + 1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url_evidently = \"https://github.com/evidentlyai/evidently\"\n",
    "repo_name_evidently = repo_url_evidently.rstrip('/').split('/')[-1]\n",
    "extract_dir_evidently = f\"./{repo_name_evidently}\"\n",
    "\n",
    "repo_url_dh = \"https://github.com/scc-digitalhub/digitalhub-tutorials/\"\n",
    "# https://github.com/scc-digitalhub/digitalhub-tutorials/tree/main\n",
    "repo_name_dh= repo_url_dh.rstrip('/').split('/')[-1]\n",
    "extract_dir_dh = f\"./{repo_name_dh}\"\n",
    "\n",
    "download_repo = True\n",
    "if download_repo:\n",
    "    #download_and_extract_repo(repo_url_evidently, extract_dir_evidently)\n",
    "    download_and_extract_repo(repo_url_dh, extract_dir_dh)\n",
    "\n",
    "#py_files = get_py_files(extract_dir_evidently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: load documents\n",
    "def split_into_chunks(repo_folder):\n",
    "    #loader = DirectoryLoader('model_cards/', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "    loader = DirectoryLoader(f\"{repo_folder}/\", glob=\"**/*.py\", loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    # Step 1.2: split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def split_into_chunks_documentation(web_url):\n",
    "    loader = WebBaseLoader(web_url)\n",
    "    documents = loader.load()\n",
    "    # Step 1.2: split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "#chunks_evidently = split_into_chunks(extract_dir_evidently)\n",
    "chunks_dh = split_into_chunks(extract_dir_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "# Step 1.3: encode chunks into vectors and store in a vector database\n",
    "#vectordb = FAISS.load_local(\n",
    "#    \"../vectorstore_evidently.db\", embeddings, allow_dangerous_deserialization=True\n",
    "#)\n",
    "#vectordb = FAISS.load_local(\n",
    "#    \"../vectorstore_dh.db\", embeddings, allow_dangerous_deserialization=True\n",
    "#)\n",
    "#vectordb_evidently = FAISS.from_documents(chunks_evidently, embeddings)\n",
    "#vectordb_evidently.save_local(\"vectorstore_evidently.db\")\n",
    "\n",
    "vectordb_dh = FAISS.from_documents(chunks_dh, embeddings)\n",
    "vectordb_dh.save_local(\"vectorstore_dh.db\")\n",
    "\n",
    "# Merge vector stores\n",
    "#vectordb_evidently.merge_from(vectordb_dh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"Please generate a new python script that detects data drift for tabular data using your context?\"\n",
    "query = \"Please generate two functions that transform the following AI operation function into the proper syntax within the AI governance platform using your context. \\\n",
    "The first function serves as the handler and the second one as the deployer of the handler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "f\"--logging_steps={logging_steps}\",\n",
      "        f\"--learning_rate={learning_rate}\",\n",
      "        f\"--warmup_steps={warmup_steps}\",\n",
      "        f\"--eval_strategy=steps\",\n",
      "        f\"--eval_steps={eval_steps}\",\n",
      "        f\"--save_strategy=steps\",\n",
      "        f\"--save_steps={save_steps}\",\n",
      "        f\"--save_total_limit=1\",\n",
      "        f\"--generation_max_length={max_sequence_length}\",\n",
      "        f\"--preprocessing_num_workers=16\",\n",
      "        f\"--max_duration_in_seconds=30\",\n",
      "        f\"--text_column_name=sentence\",\n",
      "Metadata: {'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s8-whisper-fine-tuning/src/fine_tuning_seq2seq.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "def pipeline():\n",
      "    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"employees\")) as w:\n",
      "        with DAG(name=\"dag\"):\n",
      "            A = step(\n",
      "                template={\n",
      "                    \"action\": \"transform\",\n",
      "                    \"inputs\": {\"employees\": \"{{workflow.parameters.employees}}\"},\n",
      "                    \"outputs\": {\"output_table\": \"department-50\"},\n",
      "                },\n",
      "                function=\"transform-employees\",\n",
      "            )\n",
      "    return w\n",
      "Metadata: {'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s2-dbt/src/pipeline.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "from digitalhub_runtime_hera.dsl import step\n",
      "from hera.workflows import Steps, Workflow\n",
      "\n",
      "\n",
      "def pipeline():\n",
      "    with Workflow(entrypoint=\"dag\") as w:\n",
      "        with Steps(name=\"dag\"):\n",
      "            A = step(\n",
      "                template={\"action\": \"job\"},\n",
      "                function=\"train-mlflow-model\",\n",
      "                outputs=[\"model\"],\n",
      "            )\n",
      "    return w\n",
      "Metadata: {'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s4-mlflow/src/pipeline.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "import bs4\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "from langchain_postgres import PGVector\n",
      "import os\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from openai import OpenAI\n",
      "\n",
      "from langchain import hub\n",
      "from langchain_core.documents import Document\n",
      "from typing_extensions import List, TypedDict\n",
      "\n",
      "from langgraph.graph import START, StateGraph\n",
      "from langchain.chat_models import init_chat_model\n",
      "Metadata: {'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s7-rag/src/serve.py'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Retrieval: retrieve the Top k chunks most relevant to the question based on semantic similarity.\n",
    "retriever = vectordb_dh.as_retriever()\n",
    "docs = retriever.invoke(query)\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "def pipeline():\n",
      "    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"employees\")) as w:\n",
      "        with DAG(name=\"dag\"):\n",
      "            A = step(\n",
      "                template={\n",
      "                    \"action\": \"transform\",\n",
      "                    \"inputs\": {\"employees\": \"{{workflow.parameters.employees}}\"},\n",
      "                    \"outputs\": {\"output_table\": \"department-50\"},\n",
      "                },\n",
      "                function=\"transform-employees\",\n",
      "            )\n",
      "    return w\n",
      "Metadata: {'id': 1, 'relevance_score': np.float32(0.00085208216), 'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s2-dbt/src/pipeline.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "from digitalhub_runtime_hera.dsl import step\n",
      "from hera.workflows import Steps, Workflow\n",
      "\n",
      "\n",
      "def pipeline():\n",
      "    with Workflow(entrypoint=\"dag\") as w:\n",
      "        with Steps(name=\"dag\"):\n",
      "            A = step(\n",
      "                template={\"action\": \"job\"},\n",
      "                function=\"train-mlflow-model\",\n",
      "                outputs=[\"model\"],\n",
      "            )\n",
      "    return w\n",
      "Metadata: {'id': 2, 'relevance_score': np.float32(0.0008271237), 'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s4-mlflow/src/pipeline.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "f\"--logging_steps={logging_steps}\",\n",
      "        f\"--learning_rate={learning_rate}\",\n",
      "        f\"--warmup_steps={warmup_steps}\",\n",
      "        f\"--eval_strategy=steps\",\n",
      "        f\"--eval_steps={eval_steps}\",\n",
      "        f\"--save_strategy=steps\",\n",
      "        f\"--save_steps={save_steps}\",\n",
      "        f\"--save_total_limit=1\",\n",
      "        f\"--generation_max_length={max_sequence_length}\",\n",
      "        f\"--preprocessing_num_workers=16\",\n",
      "        f\"--max_duration_in_seconds=30\",\n",
      "        f\"--text_column_name=sentence\",\n",
      "Metadata: {'id': 0, 'relevance_score': np.float32(0.00079417403), 'source': 'digitalhub-tutorials/digitalhub-tutorials-main/s8-whisper-fine-tuning/src/fine_tuning_seq2seq.py'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "\n",
    "compressor = FlashrankRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "pretty_print_docs(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant that generates Python code based on the provided context.\\nConsider the following definitions:\\n    1) The AI governance platform serves as the environment that converts a Python script into an executable function, integrating it into a larger system that orchestrates the tools and processes required to run, manage, and deploy the various components of an AI product.\\n    2) AI operation is a phase whithin the AI product lifecycle — covering activities related to the data preparation, model training or model serving and monitoring.\\n    3) The Deployer function converts the AI operation function into the syntax required by the AI governance platform.\\nIMPORTANT: If you don\\'t know the answer, just say that you don\\'t know. \\n\\n \\n Question: {question} \\n\\ndef bias_mitigation_pre_reweighing(data: Data, config: Configuration) -> Data:\\n    \"\"\"Reweighing is a pre-processing bias mitigation technique that amends the dataset to achieve statistical parity. \\n    This method adjusts the weights of the samples in the dataset to compensate for imbalances between \\n    different groups. By applying appropriate weights to each instance, \\n    it ensures that the model is not biased towards any particular group, thereby promoting fairness. \\n    The goal is to adjust the influence of each group so that the final model satisfies fairness criteria \\n    such as statistical parity or disparate impact.\"\"\"\\n    # Initialise and fit the Reweighing model to mitigate bias\\n    rew = Reweighing()\\n\\n    data = data.get_dataset()\\n    # Split the data into training and testing sets (70% training, 30% testing)\\n    data_train, data_test = train_test_split(data, test_size=config.test_size, random_state=config.random_state)\\n    # Get the feature matrix (X), target labels (y), and demographic data for both sets\\n    X_train, y_train, dem_train = split_data_from_df(data_train)\\n    X_test, y_test, dem_test = split_data_from_df(data_test)\\n    \\n    # Define the groups (Black and White) in the training data based on the \\'Ethnicity\\' column\\n    group_a_train = (dem_train[\\'Ethnicity\\'] == \\'Black\\')  # Group A: Black ethnicity\\n    group_b_train = (dem_train[\\'Ethnicity\\'] == \\'White\\')  # Group B: White ethnicity\\n\\n    # Fit the reweighing technique to adjust sample weights\\n    rew.fit(y_train, group_a_train, group_b_train)\\n\\n    # Extract the calculated sample weights from the reweighing model\\n    sample_weights = rew.estimator_params[\"sample_weight\"]\\n    data_train[\\'sample_weights\\'] = sample_weights\\n    \\n    data_train.to_parquet(config.resulting_filepath)\\n    return Data(config.resulting_filepath, data_train)\\n\\n    \\nContext: {context} \\n\\nOutput Format: Ensure your output strictly contains only the python code for transforming python functions into the syntax that digitalhub platform accepts.\\nImportant Notes: Do not report your reasoning steps or any preamble like \\'Here is the output\\', ONLY the python code.\\n*** Double Check your output that it contains only the requested python code and nothing else. ***'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(model=\"gpt-4o\")\n",
    "template = generator.format_prompt_codegen(\n",
    "    system_prompt_path=\"prompts/deployer_prompts/system_prompt_codegen.txt\", \n",
    "    user_prompt_path=\"prompts/deployer_prompts/user_prompt_codegen.txt\")\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful assistant that generates Python code based on the provided context.\\nConsider the following definitions:\\n    1) The AI governance platform serves as the environment that converts a Python script into an executable function, integrating it into a larger system that orchestrates the tools and processes required to run, manage, and deploy the various components of an AI product.\\n    2) AI operation is a phase whithin the AI product lifecycle — covering activities related to the data preparation, model training or model serving and monitoring.\\n    3) The Deployer function converts the AI operation function into the syntax required by the AI governance platform.\\nIMPORTANT: If you don\\'t know the answer, just say that you don\\'t know. \\n\\n \\n Question: {question} \\n\\ndef bias_mitigation_pre_reweighing(data: Data, config: Configuration) -> Data:\\n    \"\"\"Reweighing is a pre-processing bias mitigation technique that amends the dataset to achieve statistical parity. \\n    This method adjusts the weights of the samples in the dataset to compensate for imbalances between \\n    different groups. By applying appropriate weights to each instance, \\n    it ensures that the model is not biased towards any particular group, thereby promoting fairness. \\n    The goal is to adjust the influence of each group so that the final model satisfies fairness criteria \\n    such as statistical parity or disparate impact.\"\"\"\\n    # Initialise and fit the Reweighing model to mitigate bias\\n    rew = Reweighing()\\n\\n    data = data.get_dataset()\\n    # Split the data into training and testing sets (70% training, 30% testing)\\n    data_train, data_test = train_test_split(data, test_size=config.test_size, random_state=config.random_state)\\n    # Get the feature matrix (X), target labels (y), and demographic data for both sets\\n    X_train, y_train, dem_train = split_data_from_df(data_train)\\n    X_test, y_test, dem_test = split_data_from_df(data_test)\\n    \\n    # Define the groups (Black and White) in the training data based on the \\'Ethnicity\\' column\\n    group_a_train = (dem_train[\\'Ethnicity\\'] == \\'Black\\')  # Group A: Black ethnicity\\n    group_b_train = (dem_train[\\'Ethnicity\\'] == \\'White\\')  # Group B: White ethnicity\\n\\n    # Fit the reweighing technique to adjust sample weights\\n    rew.fit(y_train, group_a_train, group_b_train)\\n\\n    # Extract the calculated sample weights from the reweighing model\\n    sample_weights = rew.estimator_params[\"sample_weight\"]\\n    data_train[\\'sample_weights\\'] = sample_weights\\n    \\n    data_train.to_parquet(config.resulting_filepath)\\n    return Data(config.resulting_filepath, data_train)\\n\\n    \\nContext: {context} \\n\\nOutput Format: Ensure your output strictly contains only the python code for transforming python functions into the syntax that digitalhub platform accepts.\\nImportant Notes: Do not report your reasoning steps or any preamble like \\'Here is the output\\', ONLY the python code.\\n*** Double Check your output that it contains only the requested python code and nothing else. ***'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:5173/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:5173/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.440328 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:5173/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:5173/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generation: input the original question and the retrieved chunks together into LLM to generate the final answer.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.5, callbacks=[opik_tracer])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "result = rag_chain.invoke(query, callbacks=[opik_tracer])\n",
    "current_time = datetime.now()\n",
    "with open(f\"results/generated_{current_time}.py\", \"w\") as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
