{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from download_cards import download_model_cards\n",
    "from utils import *\n",
    "from huggingface_hub import login\n",
    "\n",
    "from retriever import Retriever\n",
    "from generator import Generator\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i + 1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url_evidently = \"https://github.com/evidentlyai/evidently\"\n",
    "repo_name_evidently = repo_url_evidently.rstrip('/').split('/')[-1]\n",
    "extract_dir_evidently = f\"./{repo_name_evidently}\"\n",
    "\n",
    "repo_url_dh = \"https://scc-digitalhub.github.io/docs/\"\n",
    "# https://github.com/scc-digitalhub/digitalhub-tutorials/tree/main\n",
    "\n",
    "download_repo = True\n",
    "if download_repo:\n",
    "    download_and_extract_repo(repo_url_evidently, extract_dir_evidently)\n",
    "\n",
    "#py_files = get_py_files(extract_dir_evidently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: load documents\n",
    "def split_into_chunks(repo_folder):\n",
    "    #loader = DirectoryLoader('model_cards/', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "    loader = DirectoryLoader(f\"{repo_folder}/\", glob=\"**/*.py\", loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    # Step 1.2: split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def split_into_chunks_documentation(web_url):\n",
    "    loader = WebBaseLoader(web_url)\n",
    "    documents = loader.load()\n",
    "    # Step 1.2: split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "chunks_evidently = split_into_chunks(extract_dir_evidently)\n",
    "chunks_dh = split_into_chunks_documentation(repo_url_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "# Step 1.3: encode chunks into vectors and store in a vector database\n",
    "vectordb_evidently = FAISS.load_local(\n",
    "    \"vectorstore_evidently.db\", embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "#vectordb_evidently = FAISS.from_documents(chunks_evidently, embeddings)\n",
    "#vectordb_evidently.save_local(\"vectorstore_evidently.db\")\n",
    "\n",
    "#vectordb_dh = FAISS.from_documents(chunks_dh, embeddings)\n",
    "#vectordb_dh.save_local(\"vectorstore_dh.db\")\n",
    "\n",
    "# Merge vector stores\n",
    "#vectordb_evidently.merge_from(vectordb_dh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please generate a new python script that detects data drift for tabular data using your context?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"\"\"Methods and types for data drift calculations.\"\"\"\n",
      "\n",
      "from dataclasses import dataclass\n",
      "from typing import Dict\n",
      "from typing import List\n",
      "from typing import Optional\n",
      "from typing import Sequence\n",
      "from typing import Tuple\n",
      "from typing import Union\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "Metadata: {'source': 'evidently/evidently-main/src/evidently/legacy/calculations/data_drift.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "features: Dict[str, ColumnDriftParameter]\n",
      "\n",
      "    @classmethod\n",
      "    def from_data_drift_table(cls, table: DataDriftTableResults, condition: TestValueCondition):\n",
      "        return ColumnsDriftParameters(\n",
      "            features={\n",
      "                feature: ColumnDriftParameter.from_metric(data) for feature, data in table.drift_by_columns.items()\n",
      "            },\n",
      "            condition=condition,\n",
      "        )\n",
      "Metadata: {'source': 'evidently/evidently-main/src/evidently/legacy/tests/data_drift_tests.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "from evidently.legacy.calculations.data_drift import ScatterField\n",
      "from evidently.legacy.calculations.data_drift import get_dataset_drift\n",
      "from evidently.legacy.metric_results import DatasetColumns\n",
      "from evidently.legacy.metric_results import DatasetUtilityColumns\n",
      "from evidently.legacy.metric_results import ScatterAggField\n",
      "from evidently.legacy.options.data_drift import DataDriftOptions\n",
      "from evidently.legacy.pipeline.column_mapping import ColumnMapping\n",
      "Metadata: {'source': 'evidently/evidently-main/src/evidently/legacy/spark/calculations/data_drift.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "from evidently.legacy.metrics import DatasetDriftMetric\n",
      "from evidently.legacy.metrics.data_drift.embedding_drift_methods import DriftMethod\n",
      "from evidently.legacy.model.widget import BaseWidgetInfo\n",
      "from evidently.legacy.options.data_drift import DataDriftOptions\n",
      "from evidently.metrics import ValueDrift\n",
      "from evidently.metrics.column_statistics import DriftedColumnsCount\n",
      "Metadata: {'source': 'evidently/evidently-main/src/evidently/presets/drift.py'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Retrieval: retrieve the Top k chunks most relevant to the question based on semantic similarity.\n",
    "retriever = vectordb_evidently.as_retriever()\n",
    "docs = retriever.invoke(query)\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n",
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:15<00:00, 6.71MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "features: Dict[str, ColumnDriftParameter]\n",
      "\n",
      "    @classmethod\n",
      "    def from_data_drift_table(cls, table: DataDriftTableResults, condition: TestValueCondition):\n",
      "        return ColumnsDriftParameters(\n",
      "            features={\n",
      "                feature: ColumnDriftParameter.from_metric(data) for feature, data in table.drift_by_columns.items()\n",
      "            },\n",
      "            condition=condition,\n",
      "        )\n",
      "Metadata: {'id': 1, 'relevance_score': np.float32(0.9961698), 'source': 'evidently/evidently-main/src/evidently/legacy/tests/data_drift_tests.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"\"\"Methods and types for data drift calculations.\"\"\"\n",
      "\n",
      "from dataclasses import dataclass\n",
      "from typing import Dict\n",
      "from typing import List\n",
      "from typing import Optional\n",
      "from typing import Sequence\n",
      "from typing import Tuple\n",
      "from typing import Union\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "Metadata: {'id': 0, 'relevance_score': np.float32(0.99366033), 'source': 'evidently/evidently-main/src/evidently/legacy/calculations/data_drift.py'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "from evidently.legacy.metrics import DatasetDriftMetric\n",
      "from evidently.legacy.metrics.data_drift.embedding_drift_methods import DriftMethod\n",
      "from evidently.legacy.model.widget import BaseWidgetInfo\n",
      "from evidently.legacy.options.data_drift import DataDriftOptions\n",
      "from evidently.metrics import ValueDrift\n",
      "from evidently.metrics.column_statistics import DriftedColumnsCount\n",
      "Metadata: {'id': 3, 'relevance_score': np.float32(0.9836352), 'source': 'evidently/evidently-main/src/evidently/presets/drift.py'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "\n",
    "compressor = FlashrankRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "pretty_print_docs(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant that generates Python code based on the provided context.\\nConsider the following definitions:\\n    1) Data validation is an operation in the AI system lifecycle that validates the quality of training data to identify bias issues that could impact model performance across groups of sensitive features.\\n    2) Data preprocessing is an operation in the AI system lifecycle that applies data cleaning procedure, data augmentation, type conversion. This operation can analyze data distribution, evaluate bias or discrimination issues on data and transform data in order to mitigate potential risks deriving from low data quality.\\n    3) Fairness means ensuring equity in the decision-making process of a machine learning algorithm across individuals and groups. Group fairness split a population into groups defined by protected attributes (e.g. gender, race) and seeks for some measure to be as equal as possible across groups. Some fairness metrics for measuring group fairness are: statistical parity, disparate impact, and equal opportunity difference.\\nIMPORTANT: If you don't know the answer, just say that you don't know.  \\n Question: {question} \\nContext: {context} \\n\\nOutput Format: Ensure your output strictly contains only the python code for the specified task.\\n\\nImportant Notes: Do not report your reasoning steps or any preamble like 'Here is the output', ONLY the python code.\\n\\n*** Double Check your output that it contains only the requested python code and nothing else. ***\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(model=\"gpt-4o\")\n",
    "template = generator.format_prompt_codegen(\n",
    "    system_prompt_path=\"prompts/system_prompt_codegen.txt\", \n",
    "    user_prompt_path=\"prompts/user_prompt_codegen.txt\")\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are a helpful assistant that generates Python code based on the provided context.\\nConsider the following definitions:\\n    1) Data validation is an operation in the AI system lifecycle that validates the quality of training data to identify bias issues that could impact model performance across groups of sensitive features.\\n    2) Data preprocessing is an operation in the AI system lifecycle that applies data cleaning procedure, data augmentation, type conversion. This operation can analyze data distribution, evaluate bias or discrimination issues on data and transform data in order to mitigate potential risks deriving from low data quality.\\n    3) Fairness means ensuring equity in the decision-making process of a machine learning algorithm across individuals and groups. Group fairness split a population into groups defined by protected attributes (e.g. gender, race) and seeks for some measure to be as equal as possible across groups. Some fairness metrics for measuring group fairness are: statistical parity, disparate impact, and equal opportunity difference.\\nIMPORTANT: If you don't know the answer, just say that you don't know.  \\n Question: {question} \\nContext: {context} \\n\\nOutput Format: Ensure your output strictly contains only the python code for the specified task.\\n\\nImportant Notes: Do not report your reasoning steps or any preamble like 'Here is the output', ONLY the python code.\\n\\n*** Double Check your output that it contains only the requested python code and nothing else. ***\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147302/1329418218.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.5)\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generation: input the original question and the retrieved chunks together into LLM to generate the final answer.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.5)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "result = rag_chain.invoke(query)\n",
    "current_time = datetime.now()\n",
    "with open(f\"results/generated_{current_time}.py\", \"w\") as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In a typical RAG architecture, the retriever part gets chunks of data based on a similarity search from documents stored in a knowledge base. \n",
    "But are those chunks always the most relevant for our case? (Context relevance)\n",
    "    -Reranker: reduce mismatches by using a ranker that can select the most relevant context chunks, ensuring that the LLM gets the best possible information for generating accurate answers.\n",
    "        Techniques for reranking:\n",
    "            - Contextual embeddings\n",
    "Metrics used:\n",
    "    - BERTScore\n",
    "    - Precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Instantiate the models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create the TestsetGenerator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Call the generator\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "data_transformed, \n",
    "test_size=20, \n",
    "distributions={ \n",
    "simple: 0.5, \n",
    "reasoning: 0.25, \n",
    "multi_context: 0.25}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
